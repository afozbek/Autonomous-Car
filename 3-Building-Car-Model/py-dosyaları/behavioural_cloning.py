# -*- coding: utf-8 -*-
"""Behavioural Cloning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10Wh2eDGzgNVl3LoPWsN2lOvGjS_ZVDYp
"""

#!git clone https://github.com/afozbek/Track_Data

#!ls Track_Data

import os
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import keras
from keras.models import Sequential
from keras.optimizers import Adam
from keras.layers import Convolution2D, MaxPooling2D, Dropout, Flatten, Dense
from sklearn.utils import shuffle
from sklearn.model_selection import train_test_split
import cv2
import pandas as pd
import ntpath
import random

datadir = 'Track_Data'
columns = ['center', 'left', 'right',
           'steering', 'throttle', 'reverse', 'speed']
data = pd.read_csv(os.path.join(datadir, 'driving_log.csv'),
             names=columns)
pd.set_option('display.max_colwidth', -1)
data.head()

def path_leaf(path):
  head, tail = ntpath.split(path)
  return tail
data['center'] = data['center'].apply(path_leaf)
data['left'] = data['left'].apply(path_leaf)
data['right'] = data['right'].apply(path_leaf)
data.head()

num_bins = 25
samples_per_bin = 200
hist, bins = np.histogram(data['steering'], num_bins)
center = (bins[:-1] + bins[1:]) * 0.5
plt.bar(center, hist, width = 0.05)
plt.plot((np.min(data['steering']), np.max(data['steering'])),
        (samples_per_bin, samples_per_bin))

print('total data:', len(data))
remove_list = []
for j in range(num_bins):
  list_ = []
  for i in range(len(data['steering'])):
    if data['steering'][i] >= bins[j] and data['steering'][i] <= bins[j + 1]:
      list_.append(i)
  list_ = shuffle(list_)
  list_ = list_[samples_per_bin:]
  remove_list.extend(list_)

print('removed', len(remove_list))
data.drop(data.index[remove_list], inplace=True)
print('remaining', len(data))

hist, _ = np.histogram(data['steering'], (num_bins))
plt.bar(center, hist, width = 0.05)
plt.plot((np.min(data['steering']), np.max(data['steering'])),
        (samples_per_bin, samples_per_bin))

print(data.iloc[1]) #iloc-->records based on the indexes
def load_img_steering(datadir, data):
  image_path = []
  steering = []
  for i in range(len(data)):
    indexed_data = data.iloc[i]
    center, left, right = indexed_data[0], indexed_data[1], indexed_data[2]
    image_path.append(os.path.join(datadir, center.strip()))
    steering.append(float(indexed_data[3]))
  #Convert the input to an array.  
  image_paths = np.asarray(image_path) 
  steerings = np.asarray(steering)
  return image_paths, steerings

image_paths, steerings = load_img_steering(datadir + '/IMG', data)

#Split arrays or matrices into random train and test subsets
X_train, X_valid, y_train, y_valid = train_test_split(image_paths, steerings, test_size=0.2, random_state=6)

print('Training Samples: {}\nValid Samples: {}'.format(len(X_train), len(X_valid)))

fig, axes = plt.subplots(1, 2, figsize=(12, 4))
axes[0].hist(y_train, bins=num_bins, width=0.05, color='blue')
axes[0].set_title('Training set')
axes[1].hist(y_valid, bins=num_bins, width=0.05, color='red')
axes[1].set_title('Valid set')

def img_preprocess(img_path):
  img = mpimg.imread(img_path)
  img = img[60:136, :,:]
  img = cv2.cvtColor(img, cv2.COLOR_RGB2YUV)
  img = cv2.GaussianBlur(img, (3, 3), 0)
  img = cv2.resize(img, (200, 66))
  img = img / 255
  return img

image_path = image_paths[60]
original_image = mpimg.imread(image_path)
preprocessed_image = img_preprocess(image_path)

fig, axes = plt.subplots(1, 2, figsize=(15, 10))
fig.tight_layout()
axes[0].imshow(original_image)
axes[0].set_title('Original Image')
axes[1].imshow(preprocessed_image)
axes[1].set_title('Preprocessed Image')

#Make an iterator that computes the function 
#using arguments from each of the iterables. 
X_train = np.array(list(map(img_preprocess, X_train))) #Each image in X_traing goes as parameter
X_valid = np.array(list(map(img_preprocess, X_valid))) #Each image in X_traing goes as parameter

plt.imshow(X_train[random.randint(0, len(X_train) - 1)])
plt.axis("off")
print(X_train.shape)

"""92

**A "dead" ReLU** always outputs the same value (zero as it happens, but that is not important) for any input. Probably this is arrived at by learning a large negative bias term for its weights.

In turn, that means that it takes no role in discriminating between inputs. For classification, you could visualise this as a decision plane outside of all possible input data.

Once a ReLU ends up in this state, it is unlikely to recover, because the function gradient at 0 is also 0, so gradient descent learning will not alter the weights. "Leaky" ReLUs with a small positive gradient for negative inputs (y=0.01x when x < 0 say) are one attempt to address this issue and give a chance to recover.

The **sigmoid and tanh** neurons can suffer from similar problems as their values saturate, but there is always at least a small gradient allowing them to recover in the long term.
"""

def nvidia_model():
  model = Sequential()
  #subsample => tuple of length 2. Factor by which to subsample output. Also called strides elsewhere.
  
  model.add(Convolution2D(24, 5, 5, subsample=(2, 2), input_shape=(66, 200, 3), activation='elu'))
  model.add(Convolution2D(36, 5, 5, subsample=(2, 2), activation='elu'))
  model.add(Convolution2D(48, 5, 5, subsample=(2, 2), activation='elu'))
  model.add(Convolution2D(64, 3, 3, activation='elu')) #division decreased significantly so we will not use subsample
  model.add(Convolution2D(64, 3, 3, activation='elu')) #division decreased significantly so we will not use subsample
  model.add(Dropout(rate=0.5)) #reduce overfitting
  
  model.add(Flatten())
  model.add(Dense(units=100, activation='elu')) #fully connected layer, 100 nodes
  model.add(Dropout(rate=0.5)) #reduce overfitting
  
  model.add(Dense(units=50,  activation='elu')) #fully connected layer ,  50 nodes
  model.add(Dropout(rate=0.5)) #reduce overfitting

  model.add(Dense(units=10,  activation='elu')) #fully connected layer ,  10 nodes
  model.add(Dropout(rate=0.5)) #reduce overfitting

  model.add(Dense(1))
  
  adam = Adam(lr=1e-3) #0.001
  model.compile(loss='mse', optimizer=adam)
  
  return model

model = nvidia_model()
print(model.summary())

history = model.fit(X_train, y_train, epochs=30, validation_data=(X_valid,y_valid),
          batch_size=100, verbose=1, shuffle=1)

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.legend(['Training', 'Validation'])
plt.title('Loss')
plt.xlabel('Epoch')

#model.save('model.h5')
#from google.colab import files
#files.download('model.h5')

